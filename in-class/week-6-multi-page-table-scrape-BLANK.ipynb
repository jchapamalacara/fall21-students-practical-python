{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "week-6-multi-page-table scrape_BLANK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rg5ypHYQ2vB"
      },
      "source": [
        "# Multipage Tables Scrape Demo\n",
        "\n",
        "You're often going to encounter data and tables spread across hundreds if not thousands of pages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FAa4EkaQ2vH"
      },
      "source": [
        "We're going to scrape as a demo a table that runs across several pages on this mock website.\n",
        "\n",
        "```https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTY4tDkVQ2vI"
      },
      "source": [
        "To capture your target information into a single CSV file will require the use of many of the foundational skills we've covered, including:\n",
        "\n",
        "- ```delays```\n",
        "- ```conditional logic```\n",
        "- ```while loops```\n",
        "- ```BeautifulSoup```\n",
        "\n",
        "\n",
        "And we'll explore a few new functional Python methods today."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHxjNFUQ2vI"
      },
      "source": [
        "## Scraping Strategies\n",
        "\n",
        "- How do we approach this scrape?\n",
        "- What pattern do we see?\n",
        "- How do we capture a table on a single page?\n",
        "- How do we capture a sequence of tables?\n",
        "- How we navigate from page 1 to the subsequent pages?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9IgexKhQ2vJ"
      },
      "source": [
        "# Let's code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ6RZpiseWIm"
      },
      "source": [
        "## pip install icecream for debugging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkvOnsTvN0Bu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ7ZNO8YQ2vJ"
      },
      "source": [
        "# import libraries\n",
        "\n",
        "from bs4 import BeautifulSoup  ## web scraping\n",
        "import requests ## request html for a page(s)\n",
        "import pandas as pd ## pandas to work with data\n",
        "from icecream import ic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl2yMzxEQ2vK"
      },
      "source": [
        "## Single Table Scrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3eCtCpJQ2vK"
      },
      "source": [
        "##scrape url website\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PShccOFp4Xg7"
      },
      "source": [
        "## page content type\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEbE38UkHkxb"
      },
      "source": [
        "## page text type\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq7Fxh77KLU8"
      },
      "source": [
        "## Since ```page.text``` returns a ```str```, we don't need to use ```BeautifulSoup```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh83GGbyQ2vL"
      },
      "source": [
        "## use Pandas to read tables on page\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvtGiiY7Q2vL"
      },
      "source": [
        "## Do we want the first table?\n",
        "# type(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXIYAPPqQ2vL"
      },
      "source": [
        "## store it into a copy called animals_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gheKHsbiQ2vM"
      },
      "source": [
        "## But we want to scrape multiple pages\n",
        "2 ways to build a list of urls that we have to navigate to:\n",
        "\n",
        "1. Placeholders\n",
        "2. f-strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCu3yvc6GsF"
      },
      "source": [
        "## Never do this manually\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVz1WRdPQ2vM"
      },
      "source": [
        "### 1. Placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfuCNbRFQ2vM"
      },
      "source": [
        "## How is it different?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROt6YrlcQ2vM"
      },
      "source": [
        "## Placeholders\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sandeepmj/scrape-example-page/master/images/placeholder1.png\" style=\"width:500px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZXNStCMQ2vN"
      },
      "source": [
        "## Placeholders\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sandeepmj/scrape-example-page/master/images/placeholder2.png\" style=\"width:500px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zi6fkagQ2vN"
      },
      "source": [
        "## Placeholders\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/sandeepmj/scrape-example-page/master/images/placeholder3.png\" style=\"width:500px;\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs5WSXMGQ2vN"
      },
      "source": [
        "## Filling the Placeholder\n",
        "\n",
        "### We use ```.format()``` to fill in values into the ```{}```placeholder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk7B2WjiQ2vN"
      },
      "source": [
        "## here's our base url\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvEyUA0aQ2vN"
      },
      "source": [
        "## Using a ```for loop```\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrgrcSVYQ2vO"
      },
      "source": [
        "## using list comprehension\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW-gnRagQ2vO"
      },
      "source": [
        "### 2. Using f-strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8hJx1nSQ2vO"
      },
      "source": [
        "## base url of site to scrape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGQMfxyrQ2vO"
      },
      "source": [
        "## Using a ```for loop```\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WbAbHYLVcfX"
      },
      "source": [
        "## using list comprehension\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrJexdPh-JIq"
      },
      "source": [
        "## f string base url\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lZUHi32-Sp7"
      },
      "source": [
        "## using list comprehension\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_oDySdwQ2vO"
      },
      "source": [
        "## Back to our scrape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLsGGNjTQ2vP"
      },
      "source": [
        "## let's remind ourselves of url variable's value\n",
        "\n",
        "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElvFr3AQQ2vP"
      },
      "source": [
        "## We know we need a placeholder value of upto ```4```\n",
        "## Let's create a variable called  ```total_pages``` to match number of pages on site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6Vz-D3LQ2vP"
      },
      "source": [
        "## total pages to scrape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcGXP_Nn_juK"
      },
      "source": [
        "## generates urls and loop through to get response from surver (are you getting 200?)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51i7AzHFQ2vP"
      },
      "source": [
        "# We have a problem...\n",
        "\n",
        "### We're hitting the server way too fast. We have to add a delay before we proceed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyDiu5HVQ2vQ"
      },
      "source": [
        "## Let's import the required libaries to create a delay\n",
        "from random import randrange ##  allows us to randomize numbers library\n",
        "import time ## time tracker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuZ8Fw9sQ2vQ"
      },
      "source": [
        "## Let's run our code again but with appropriate delay\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOnd8mhPDUR0"
      },
      "source": [
        "## let's remind ourselves of url variable's value\n",
        "\n",
        "base_url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKhoz13wDEKu"
      },
      "source": [
        "## ## for loop with timer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uPWDrxJQ2vQ"
      },
      "source": [
        "## Working Around Errors\n",
        "\n",
        "When you scrape hundreds of pages, there's chance that one of the URLs might be a dud.\n",
        "\n",
        "We can set up a error control to see what kind of responses we get:\n",
        "\n",
        "```<Response [200]>``` means website is accessible.\n",
        "\n",
        "```<Response [404]>``` means broken link or no page on content.\n",
        "\n",
        "In that case, your whole code might break and you'll have to figure out where it broke.\n",
        "\n",
        "We can make that easier with conditional logic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFcihmt6Q2vQ"
      },
      "source": [
        "## CHECK FOR ERROR\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOlkUp23h2wF"
      },
      "source": [
        "## show broken links\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gyRplcgQ2vQ"
      },
      "source": [
        "# All in One Step\n",
        "\n",
        "Because we are using a  ```for loop``` that cycles through each link to do multiple steps on our target data, we need to have it done as one step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJoLS9AgQ2vR"
      },
      "source": [
        "## Combined url timed nav with table scrape\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-lESTDPMP8s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emr3u2OvQ2vR"
      },
      "source": [
        "## FUNCTION to download individual dataframes in a list as a single csv\n",
        "def combine_tables(list_name,filename):\n",
        "  '''\n",
        "  Takes dataframes in a list and combines into a single CSV.\n",
        "  Tables must have identical column headers and order\n",
        "  Arguments: name of list and the CSV name you want (in quotes as a string)\n",
        "  '''\n",
        "  df = pd.concat(list_name) ## join/concat all the dataframes into one dataframe\n",
        "  df.to_csv(filename, encoding='utf-8', index=False) ## convert that single dataframe into a csv\n",
        "#   files.download(filename) ## download it\n",
        "  print(f\"{filename} is in your current folder!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPY3Cbj-Q2vR"
      },
      "source": [
        "## CALL THE FUNCTION\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57-o0t3xQ2vS"
      },
      "source": [
        "combine_tables"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}