{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1gmCxq4kQCP"
   },
   "source": [
    "# Scraping non-tabular data\n",
    "\n",
    "Look at the first section of CEO data on <a href=\"https://sandeepmj.github.io/scrape-example-page/#organized\">this page</a>:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/#organized```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfF9k_35kQCS"
   },
   "source": [
    "### This won't be as simple as scraping a table:\n",
    "\n",
    "* Where and how is the content held on the page?\n",
    "* How can we access it?\n",
    "* Is there a pattern?\n",
    "* Is there anything that breaks the pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoRTykDqkQCS"
   },
   "outputs": [],
   "source": [
    "## import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests ## a library that returns information from websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gu9tCcmGkQCT"
   },
   "outputs": [],
   "source": [
    "# url to scrape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rwRWytdkQCT"
   },
   "outputs": [],
   "source": [
    "## We use the request library to grab page content\n",
    "## get contents of URL and store in object called page.\n",
    "## Let's print page and see what we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0E4AW8fbkQCU"
   },
   "outputs": [],
   "source": [
    "## what type of object is it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6spaTdiWkQCU"
   },
   "outputs": [],
   "source": [
    "## We'll used BeautifulSoup to convert content into content we recognize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8vbDKpokQCU"
   },
   "outputs": [],
   "source": [
    "## prettify the print so it's easier to see the HTML tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1179X78kQCV"
   },
   "source": [
    "## Non-tabular data in classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPqaD4GZkQCV"
   },
   "outputs": [],
   "source": [
    "## soup captures the entire page.\n",
    "## we narrow it down to capture our target section and store in object called dis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9H6P0BLkQCV"
   },
   "outputs": [],
   "source": [
    "## find class ceo and store in object called ceos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bseM95nkQCW"
   },
   "source": [
    "### Decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWVfsdWJkQCW"
   },
   "outputs": [],
   "source": [
    "### remove spans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "getdCBY6kQCW"
   },
   "outputs": [],
   "source": [
    "## get ranks into a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHvQiwI4kQCW"
   },
   "outputs": [],
   "source": [
    "## get names into a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bcW23CskQCX"
   },
   "outputs": [],
   "source": [
    "## annual compensation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LR30fJZzkQCX"
   },
   "outputs": [],
   "source": [
    "## company name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtiX9CcgkQCX"
   },
   "outputs": [],
   "source": [
    "## Pandas to create data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "071z2sBGkQCX"
   },
   "source": [
    "## What if there are dozens or more column header names?\n",
    "### That's a lot of typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEbafU5wkQCX"
   },
   "outputs": [],
   "source": [
    "## let's quickly recreate our soup.\n",
    "## in the future we'll grab the labels in our early steps\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "organized = soup.find(\"section\", id=\"organized\")\n",
    "organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x5dWnbL0kQCY"
   },
   "outputs": [],
   "source": [
    "## find the labels (column headers)\n",
    "## using list comprehension save in a list called labels_lc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyPnVtM8kQCY"
   },
   "outputs": [],
   "source": [
    "## labels without html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebYp-dzikQCY"
   },
   "outputs": [],
   "source": [
    "## pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLrXVGABkQCY"
   },
   "outputs": [],
   "source": [
    "def export2csv(a_list, filename):\n",
    "    '''\n",
    "    provide list name first\n",
    "    provide filename as a string\n",
    "    '''\n",
    "    df = pd.DataFrame(a_list)\n",
    "    df.to_csv(filename, encoding='utf-8', index=False)\n",
    "    print(f\"{filename} is in your project folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8kGuG86kQCZ"
   },
   "outputs": [],
   "source": [
    "## export to csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXOsj1rhkQCZ"
   },
   "source": [
    "# Reality - not handed to you so cleanly\n",
    "\n",
    "### Let's scrape the disorganized section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON8edAP4kQCZ"
   },
   "outputs": [],
   "source": [
    "## soup captures the entire page.\n",
    "## we narrow it down to capture our target section and store in object called dis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfQIz-BHkQCZ"
   },
   "source": [
    "### Scrape the non-tabular data in id=\"disorganized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6CkJHJfkQCZ"
   },
   "outputs": [],
   "source": [
    "## find the labels (column headers)\n",
    "## using list comprehension save in a list called labels_lc\n",
    "labels_lc = [label.get_text().strip(':') for label in labels]\n",
    "labels_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Keuy894kQCa"
   },
   "outputs": [],
   "source": [
    "## find class ceo and store in object called ceos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ztFbtT1kQCa"
   },
   "outputs": [],
   "source": [
    "##THIS ONE REQUIRES US TO TYPE THE COLUMN HEADERS AS KEYS\n",
    "## NEXT CELL IS MORE EFFICIENT\n",
    "\n",
    "## we only printed them out. let's store in a list of dicts called ceo_dict_list\n",
    "ceo_dict_list = []\n",
    "for ceo in ceos:\n",
    "    each_ceo = ceo.find_all(\"dt\")\n",
    "    rank = each_ceo[0].get_text()\n",
    "    name = each_ceo[1].get_text()\n",
    "    annual_compensation = each_ceo[2].get_text()\n",
    "    company = each_ceo[3].get_text()\n",
    "    ceo_dict = {\"Rank\": rank, \"Name\": name, \"Company\": company, \"Annual Compensation\": annual_compensation}\n",
    "    ceo_dict_list.append(ceo_dict)\n",
    "ceo_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3Et7aYokQCa"
   },
   "outputs": [],
   "source": [
    "## we only printed them out. let's store in a list of dicts called ceo_dict_list\n",
    "ceo_dict_list2 = []\n",
    "for ceo in ceos:\n",
    "    my_list = []\n",
    "    each_ceo = ceo.find_all(\"dt\")\n",
    "    rank1 = each_ceo[0].get_text()\n",
    "    name1 = each_ceo[1].get_text()\n",
    "    annual_compensation1 = each_ceo[2].get_text()\n",
    "    company1 = each_ceo[3].get_text()\n",
    "    my_list.extend([rank1, name1, annual_compensation1, company1])\n",
    "    print(my_list)\n",
    "    ceo_dict = dict(zip(labels_lc, my_list))\n",
    "    ceo_dict_list2.append(ceo_dict)\n",
    "ceo_dict_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTxmq3ZJkQCa"
   },
   "outputs": [],
   "source": [
    "## use pandas to write to csv file\n",
    "filename = \"disorganized.csv\" ## what are file name is\n",
    "df = pd.DataFrame(ceo_dict_list2) ## we turn our list of dicts into a dataframe which we're call df\n",
    "df.to_csv(filename, encoding='utf-8', index=False) ## export to csv as utf-8 coding (it just has to be this)\n",
    "\n",
    "print(f\"{filename} is in your project folder!\") ## a print out that tells us the file is ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rAOWI7mkQCb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "week-5-inclass-non-tabular-scrape_BLANKS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
